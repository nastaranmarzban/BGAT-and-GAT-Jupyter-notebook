{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount to gdrive to read data, adjacency matrix, p_link and labels. "
      ],
      "metadata": {
        "id": "rmbiJqbeOZgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFNXYWZJMLGZ",
        "outputId": "020da975-0a99-47ae-e754-3cf664ee6456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pandas to read csv files from gdrive. Install torch and numpy which are packages that are needed durind coding."
      ],
      "metadata": {
        "id": "Iayyepa7OcnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmviDF0TMqWo",
        "outputId": "617099bd-11b4-4d1b-fda9-5b2f6a046282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data which are processed in preprocessing_data_7classes.ipynb file."
      ],
      "metadata": {
        "id": "bCHZF_7yOndI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/gdrive/MyDrive/7classes.sim/data_7classes.pt\"\n",
        "Data = torch.load(path)\n",
        "Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NENfdCeTMsRq",
        "outputId": "167f44b1-3ff8-414a-8b2b-ed686751ffe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': tensor([[-0.6680,  0.3545,  0.1382,  ...,  2.1196,  0.2813,  1.3384],\n",
              "         [ 0.0282,  1.6481,  1.9474,  ...,  0.8241,  0.9470,  1.9798],\n",
              "         [-0.4644,  0.5799,  0.0618,  ..., -0.5351, -0.2698,  0.9294],\n",
              "         ...,\n",
              "         [-0.2195, -0.8809,  0.0466,  ..., -0.5610,  2.1045, -0.2778],\n",
              "         [-0.7321,  0.0596,  1.4736,  ...,  0.7870,  0.7216, -1.5205],\n",
              "         [ 1.0196,  0.5744,  0.3943,  ...,  1.7039, -1.3692,  0.6619]]),\n",
              " 'edge_index': tensor([[ 0,  0,  0,  0,  0,  0,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  4,\n",
              "           4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
              "           6,  7,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
              "           9, 10, 10, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 14, 15, 15, 16, 16,\n",
              "          17, 17, 17, 18, 18, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21,\n",
              "          21, 21, 22, 22, 22, 22, 23, 24, 24, 24, 24, 25, 25, 26, 26, 26, 26, 27,\n",
              "          27, 27, 28, 28, 29, 29, 29, 30, 30, 30, 31, 32, 32, 32, 32, 32, 32, 33,\n",
              "          33, 33, 34, 34, 34, 35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 38, 38, 39,\n",
              "          39, 39, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 42, 42, 42, 43, 43,\n",
              "          43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 46,\n",
              "          46, 46, 46, 47, 47, 47, 48, 48, 48, 48, 49, 49, 50, 50, 50, 51, 51, 51,\n",
              "          51, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55,\n",
              "          56, 56, 56, 56, 56, 58, 58, 58, 59, 59, 60, 60, 60, 60, 61, 61, 61, 61,\n",
              "          61, 61, 61, 62, 62, 62, 62, 63, 63, 63, 64, 65, 65, 65, 66, 66, 66, 67,\n",
              "          67, 68, 68, 68, 68, 69, 69, 69],\n",
              "         [21, 22, 26, 36, 38, 65, 12, 19, 63,  3,  4, 19, 30, 42, 67,  2, 16,  2,\n",
              "          10, 16, 23, 31, 49, 51, 62, 67, 49, 60,  8,  9, 14, 20, 32, 41, 45, 53,\n",
              "          55, 22,  6,  9, 20, 32, 41, 45, 61,  6,  8, 14, 41, 42, 45, 46, 48, 53,\n",
              "          61,  4, 25, 17, 35, 47,  1, 15, 33, 58,  6,  9, 20, 61, 12, 62,  3,  4,\n",
              "          11, 22, 46, 43, 66,  1,  2, 59,  6,  8, 14, 32, 40, 41, 61,  0, 43, 44,\n",
              "          58, 63,  0,  7, 17, 47,  4, 40, 44, 54, 68, 10, 27,  0, 28, 56, 68, 25,\n",
              "          33, 60, 26, 68, 30, 35, 68,  2, 29, 35,  4,  6,  8, 20, 45, 53, 61, 13,\n",
              "          27, 50, 37, 43, 64, 11, 29, 30,  0, 43, 50, 54, 34, 51, 54,  0, 69, 43,\n",
              "          62, 66, 20, 24, 51, 54, 56,  6,  8,  9, 20, 45, 53,  2,  9, 69, 18, 21,\n",
              "          34, 36, 39, 21, 24, 46, 48, 54, 56, 65,  6,  8,  9, 32, 41, 53, 61,  9,\n",
              "          17, 44, 54, 11, 22, 50,  9, 44, 54, 56,  4,  5, 33, 36, 47,  4, 37, 40,\n",
              "          66,  6,  9, 32, 41, 45, 61, 24, 36, 37, 40, 44, 46, 48, 56, 65,  6, 60,\n",
              "          26, 40, 44, 48, 54, 13, 21, 69, 19, 60,  5, 27, 55, 59,  8,  9, 14, 20,\n",
              "          32, 45, 53,  4, 15, 39, 63,  1, 21, 62, 34,  0, 44, 54, 18, 39, 51,  2,\n",
              "           4, 24, 26, 28, 29, 38, 42, 58]]),\n",
              " 'y': tensor([0, 5, 6, 6, 6, 3, 1, 4, 1, 1, 6, 4, 5, 0, 1, 5, 6, 0, 5, 3, 1, 5, 4, 6,\n",
              "         2, 3, 4, 3, 4, 4, 6, 6, 1, 0, 3, 4, 2, 3, 0, 5, 2, 1, 0, 5, 2, 1, 2, 4,\n",
              "         2, 6, 0, 6, 4, 1, 2, 3, 2, 2, 0, 3, 3, 1, 5, 5, 3, 2, 5, 0, 4, 0]),\n",
              " 'train_mask': tensor([False, False,  True, False,  True, False, False, False, False,  True,\n",
              "         False,  True,  True,  True,  True,  True, False, False,  True, False,\n",
              "         False, False, False,  True, False, False, False, False, False,  True,\n",
              "         False, False,  True, False,  True, False,  True,  True, False, False,\n",
              "          True, False, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False,  True, False, False, False,  True,  True,\n",
              "         False, False, False, False, False, False, False,  True, False, False]),\n",
              " 'val_mask': tensor([False, False, False, False, False, False, False, False,  True, False,\n",
              "          True, False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True, False,\n",
              "         False,  True, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False,  True, False, False,  True, False]),\n",
              " 'test_mask': tensor([ True,  True, False,  True, False,  True,  True,  True, False, False,\n",
              "         False, False, False, False, False, False,  True,  True, False,  True,\n",
              "         False,  True,  True, False,  True, False,  True,  True,  True, False,\n",
              "          True,  True, False, False, False,  True, False, False,  True,  True,\n",
              "         False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
              "          True, False, False,  True, False,  True,  True,  True, False, False,\n",
              "          True,  True, False,  True,  True, False,  True, False, False,  True])}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/gdrive/MyDrive/7classes.sim/p_links_7classes.pt\"\n",
        "p_link = torch.load(path)\n",
        "print(p_link.size())\n",
        "print(p_link.dtype)\n",
        "print(p_link.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMGf5sCwM7_F",
        "outputId": "6a2c1b6d-f4d1-4ca1-bb8a-9b171042d2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([70, 70])\n",
            "torch.float32\n",
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install torch_geometic."
      ],
      "metadata": {
        "id": "BfG1_syAPI6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMnacU0EM-SS",
        "outputId": "28aacf36-15f1-4e24-b263-2e8a420b36b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create BGAT and GAT layers. "
      ],
      "metadata": {
        "id": "4C1AhUIcP7v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import MessagePassing\n",
        "import torch_geometric.nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch_geometric.nn import GATConv"
      ],
      "metadata": {
        "id": "7lu6-gqvNAw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BGATConv(MessagePassing):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(node_dim = 0, **kwargs)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index, _ = add_self_loops(Data[\"edge_index\"], num_nodes = Data[\"x\"].size(0))\n",
        "        x = self.lin(x)\n",
        "        atten = p_link\n",
        "        return self.propagate(edge_index, x=x, atten = atten)\n",
        "\n",
        "    def message(self, x_j, atten, edge_index_i, edge_index_j):\n",
        "        return atten[edge_index_i, edge_index_j].reshape(-1,1) * x_j\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)"
      ],
      "metadata": {
        "id": "myWjLG9kNH2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Myconv_BGAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(2)\n",
        "        self.conv1 = BGATConv(in_channels = 500, out_channels = 15)\n",
        "        self.conv2 =  BGATConv(15, 10)\n",
        "        self.conv3 =  BGATConv(10, 7)\n",
        "        \n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = F.torch.tanh(h)\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = F.torch.tanh(h)\n",
        "        h = self.conv3(h, edge_index)# Final GNN embedding space.\n",
        "        h = F.log_softmax(h)\n",
        "        return h\n",
        "        \n",
        "model_BGAT = Myconv_BGAT()\n",
        "print(model_BGAT)\n",
        "print(model_BGAT.forward(Data[\"x\"], Data[\"edge_index\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXi2x_WTNNyS",
        "outputId": "a87ce03c-fe0f-4608-8780-ecd5867e8319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Myconv_BGAT(\n",
            "  (conv1): BGATConv(500, 15)\n",
            "  (conv2): BGATConv(15, 10)\n",
            "  (conv3): BGATConv(10, 7)\n",
            ")\n",
            "tensor([[ -1.6038,  -3.5480,  -0.7512,  -1.4632,  -4.1339,  -3.0155,  -6.3295],\n",
            "        [ -1.8167,  -2.2806,  -2.1076,  -0.7447,  -3.9225,  -2.1941,  -4.8831],\n",
            "        [ -1.9168,  -2.9504,  -2.5775,  -0.3901,  -5.1676,  -3.2022,  -6.6732],\n",
            "        [ -1.7701,  -1.8791,  -1.9542,  -0.9096,  -3.5461,  -2.4490,  -4.0514],\n",
            "        [ -3.2950,  -2.7694,  -4.2013,  -0.1424,  -8.1786,  -4.0349, -10.3955],\n",
            "        [ -1.8265,  -2.6651,  -1.5127,  -1.7854,  -2.1845,  -1.6247,  -2.6327],\n",
            "        [ -1.9964,  -6.3456,  -8.2527,  -0.1534, -10.2619,  -5.4350, -17.6342],\n",
            "        [ -1.3819,  -2.4975,  -1.5486,  -1.5245,  -2.3928,  -2.1056,  -3.7649],\n",
            "        [ -1.8369,  -5.1594,  -7.4488,  -0.1958,  -8.7107,  -4.4240, -14.7349],\n",
            "        [ -1.3772,  -7.2671,  -8.1324,  -0.2954,  -9.9204,  -5.9984, -18.6348],\n",
            "        [ -1.7579,  -1.9779,  -1.7986,  -1.2895,  -2.9389,  -1.7297,  -4.0152],\n",
            "        [ -1.3286,  -2.8458,  -2.5069,  -0.9945,  -2.9923,  -1.7821,  -4.9385],\n",
            "        [ -1.6179,  -2.2537,  -1.9370,  -0.9566,  -3.2694,  -2.1448,  -4.3252],\n",
            "        [ -2.0413,  -3.0332,  -1.4666,  -0.8343,  -3.1194,  -2.2809,  -4.5347],\n",
            "        [ -1.4533,  -3.8429,  -4.9857,  -0.3564,  -5.9729,  -3.3494,  -9.9014],\n",
            "        [ -1.6717,  -2.2061,  -1.8361,  -0.9425,  -3.3719,  -2.2535,  -4.3052],\n",
            "        [ -1.9253,  -1.7694,  -2.0773,  -0.7827,  -4.1365,  -2.5703,  -4.7368],\n",
            "        [ -1.1448,  -2.8997,  -1.9701,  -1.1252,  -2.8680,  -2.3167,  -4.9327],\n",
            "        [ -1.4807,  -2.8264,  -1.7124,  -1.6298,  -2.2323,  -1.6243,  -3.4245],\n",
            "        [ -1.3060,  -2.7879,  -1.9878,  -1.1833,  -2.7368,  -1.9893,  -3.7856],\n",
            "        [ -2.4842,  -5.1157,  -6.7084,  -0.1063,  -9.0536,  -4.5950, -14.3814],\n",
            "        [ -2.0978,  -2.9993,  -1.6523,  -0.5494,  -5.2159,  -2.9648,  -6.4753],\n",
            "        [ -1.2924,  -3.2089,  -1.2647,  -1.1926,  -3.5439,  -2.6948,  -5.8953],\n",
            "        [ -1.9011,  -1.6156,  -2.1905,  -1.0560,  -3.1894,  -2.0869,  -3.6172],\n",
            "        [ -2.9185,  -2.6455,  -0.8620,  -0.9615,  -4.6779,  -2.8778,  -5.3275],\n",
            "        [ -1.8435,  -2.9619,  -1.1213,  -2.2711,  -2.0013,  -1.6750,  -3.2577],\n",
            "        [ -1.3834,  -2.7025,  -2.3954,  -0.9291,  -3.3032,  -1.8796,  -4.9954],\n",
            "        [ -1.8612,  -3.9022,  -0.8084,  -2.6177,  -2.0584,  -1.8979,  -3.5672],\n",
            "        [ -1.4406,  -2.5014,  -2.8307,  -1.0795,  -2.7460,  -1.5989,  -4.1229],\n",
            "        [ -1.6668,  -2.7741,  -2.5828,  -1.0732,  -2.6538,  -1.4135,  -4.0387],\n",
            "        [ -1.6283,  -2.6854,  -2.3709,  -0.8934,  -2.9766,  -1.7965,  -4.1324],\n",
            "        [ -1.7869,  -1.6578,  -2.0860,  -1.0747,  -3.2599,  -2.1568,  -3.8037],\n",
            "        [ -1.7870,  -4.9416,  -7.4840,  -0.2138,  -8.2750,  -4.0697, -13.9755],\n",
            "        [ -1.8731,  -3.4423,  -0.8125,  -1.5248,  -2.7649,  -2.5778,  -4.2680],\n",
            "        [ -1.6198,  -2.3216,  -2.9784,  -1.1123,  -2.7643,  -1.4033,  -4.1675],\n",
            "        [ -1.5603,  -2.9350,  -2.7888,  -1.0113,  -2.8771,  -1.4065,  -4.5806],\n",
            "        [ -1.9735,  -3.2781,  -0.5702,  -1.7704,  -3.7495,  -2.8536,  -5.0330],\n",
            "        [ -1.9377,  -2.0535,  -2.1419,  -1.0110,  -3.0088,  -1.7205,  -4.0133],\n",
            "        [ -2.1042,  -2.2275,  -1.2868,  -1.1681,  -3.4103,  -2.0397,  -3.9087],\n",
            "        [ -1.4393,  -2.6331,  -1.8407,  -1.2007,  -2.9226,  -1.8026,  -4.3655],\n",
            "        [ -3.0496,  -2.5458,  -1.2049,  -0.6623,  -5.2719,  -2.9603,  -6.2723],\n",
            "        [ -1.6085,  -4.8924,  -6.5262,  -0.2514,  -8.1169,  -4.3552, -14.0953],\n",
            "        [ -1.4657,  -2.8028,  -2.8830,  -0.6369,  -3.6089,  -2.3789,  -5.5673],\n",
            "        [ -0.7773,  -3.7583,  -2.5682,  -1.1438,  -3.3985,  -2.4436,  -6.5332],\n",
            "        [ -1.8613,  -3.9852,  -0.9026,  -0.9212,  -5.2803,  -4.0843,  -7.7240],\n",
            "        [ -1.3607,  -5.5228,  -7.6593,  -0.3185,  -8.3198,  -4.4600, -15.2622],\n",
            "        [ -1.5658,  -3.0897,  -1.3363,  -0.8769,  -4.0323,  -3.0785,  -5.8493],\n",
            "        [ -1.0712,  -3.1716,  -1.5207,  -1.3095,  -2.9694,  -2.6474,  -5.3476],\n",
            "        [ -1.5326,  -3.3265,  -1.1179,  -0.9841,  -4.2764,  -3.4490,  -6.3780],\n",
            "        [ -1.9321,  -1.8861,  -1.9088,  -1.0320,  -3.2191,  -2.0160,  -3.6584],\n",
            "        [ -1.3494,  -3.3667,  -0.9539,  -1.6013,  -2.8249,  -2.9543,  -4.8513],\n",
            "        [ -2.2780,  -2.0161,  -1.7277,  -0.9238,  -3.4905,  -1.9061,  -4.5570],\n",
            "        [ -1.7491,  -2.0335,  -1.8590,  -1.6914,  -2.0896,  -1.8924,  -2.5176],\n",
            "        [ -1.2946,  -5.2207,  -7.2079,  -0.3502,  -7.7845,  -4.2077, -14.3007],\n",
            "        [ -2.6476,  -5.9636,  -0.4053,  -1.3672,  -7.0811,  -5.4780, -11.5613],\n",
            "        [ -1.5830,  -2.6416,  -1.4846,  -1.6772,  -2.3483,  -1.7631,  -3.1501],\n",
            "        [ -1.9439,  -3.3653,  -0.9183,  -0.9649,  -4.6167,  -3.4871,  -6.4291],\n",
            "        [ -2.1351,  -1.9690,  -1.5963,  -2.0159,  -2.0620,  -1.7882,  -2.1905],\n",
            "        [ -1.9816,  -3.1107,  -2.4831,  -0.5013,  -3.7534,  -2.2881,  -5.6724],\n",
            "        [ -1.3460,  -3.2663,  -1.6683,  -1.8072,  -2.0982,  -1.7196,  -3.0557],\n",
            "        [ -1.9818,  -4.2484,  -0.6942,  -2.4681,  -2.4561,  -1.9247,  -3.4432],\n",
            "        [ -1.8601,  -5.7267,  -8.1357,  -0.1845,  -9.0650,  -4.6959, -15.4481],\n",
            "        [ -1.6893,  -2.4763,  -2.2446,  -0.5931,  -4.6691,  -2.7949,  -6.1122],\n",
            "        [ -1.6833,  -2.4185,  -2.5652,  -0.5826,  -4.5029,  -2.5726,  -6.0176],\n",
            "        [ -1.7815,  -1.9004,  -2.7247,  -1.2928,  -2.4638,  -1.5687,  -3.0242],\n",
            "        [ -2.8486,  -3.1134,  -0.3499,  -1.9319,  -4.6392,  -3.5031,  -4.7998],\n",
            "        [ -1.4719,  -2.7454,  -1.7647,  -1.3903,  -2.4030,  -1.7374,  -3.9309],\n",
            "        [ -1.8564,  -1.8266,  -1.7589,  -1.0306,  -3.3063,  -2.3950,  -3.6521],\n",
            "        [ -1.6585,  -3.0123,  -3.1728,  -0.7164,  -3.6098,  -1.6124,  -5.6492],\n",
            "        [ -2.0541,  -2.9838,  -2.4706,  -0.4776,  -4.0673,  -2.3390,  -5.8739]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-cedf2bd29c67>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  h = F.log_softmax(h)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Myconv_GAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(2)\n",
        "        self.conv1 = GATConv(in_channels = 500, out_channels = 50)\n",
        "        self.conv2 =  GATConv(50, 10)\n",
        "        self.conv3 =  GATConv(10, 7)\n",
        "        \n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        f = self.conv1(x, edge_index)\n",
        "        f = F.torch.tanh(f)\n",
        "        f = self.conv2(f, edge_index)\n",
        "        f = F.torch.tanh(f)\n",
        "        f = self.conv3(f, edge_index)# Final GNN embedding space.\n",
        "        f = F.log_softmax(f)\n",
        "        return f\n",
        "        \n",
        "\n",
        "model_GAT = Myconv_GAT()\n",
        "print(model_GAT)\n",
        "print(model_GAT.forward(Data[\"x\"], Data[\"edge_index\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrVHbyPbNSRy",
        "outputId": "14f2ad4d-a1ad-40dc-8907-62f76b772abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Myconv_GAT(\n",
            "  (conv1): GATConv(500, 50, heads=1)\n",
            "  (conv2): GATConv(50, 10, heads=1)\n",
            "  (conv3): GATConv(10, 7, heads=1)\n",
            ")\n",
            "tensor([[-2.4665, -2.1269, -2.5751, -1.0695, -1.9733, -1.8291, -2.5636],\n",
            "        [-1.8347, -2.6898, -2.3343, -2.1443, -1.2235, -2.2313, -1.8525],\n",
            "        [-1.7384, -2.0836, -2.0600, -1.8335, -1.9923, -1.9708, -1.9902],\n",
            "        [-1.6594, -2.1520, -1.9603, -1.3046, -2.4086, -2.3315, -2.3610],\n",
            "        [-1.9883, -2.0943, -2.3361, -1.6303, -1.9120, -1.7473, -2.0770],\n",
            "        [-1.6118, -2.5256, -2.4606, -1.6407, -1.6896, -1.9180, -2.2097],\n",
            "        [-1.8169, -1.7161, -2.2494, -2.0461, -2.2622, -1.8042, -1.8689],\n",
            "        [-2.1201, -2.3137, -1.9353, -1.6325, -1.7257, -1.7829, -2.3532],\n",
            "        [-1.8635, -1.6950, -2.2745, -2.0713, -2.2583, -1.7564, -1.8630],\n",
            "        [-2.0157, -1.7854, -2.4848, -1.8401, -2.0994, -1.6568, -1.9406],\n",
            "        [-2.0922, -2.0198, -2.5697, -1.3342, -2.1416, -1.8899, -1.9993],\n",
            "        [-1.9327, -2.5701, -2.1212, -1.4091, -1.7320, -1.9227, -2.3935],\n",
            "        [-1.8124, -2.6790, -2.1098, -2.0240, -1.3009, -2.2758, -1.9683],\n",
            "        [-2.0198, -2.0620, -2.1830, -0.8652, -2.9067, -2.4283, -2.7552],\n",
            "        [-1.9365, -1.6719, -2.3347, -1.9693, -2.3006, -1.7046, -1.9006],\n",
            "        [-1.8643, -2.5881, -2.0540, -2.0721, -1.3345, -2.2315, -1.9308],\n",
            "        [-1.7107, -2.1997, -2.0286, -1.2490, -2.3400, -2.3074, -2.3616],\n",
            "        [-2.1736, -2.4114, -2.3668, -1.3821, -1.6953, -1.7753, -2.3154],\n",
            "        [-2.3504, -2.2895, -2.7553, -2.4199, -1.2242, -1.7771, -1.6728],\n",
            "        [-1.7447, -2.5712, -2.3711, -2.1689, -1.3456, -2.1259, -1.8234],\n",
            "        [-1.9896, -1.6576, -2.3036, -1.9400, -2.2829, -1.6958, -1.9406],\n",
            "        [-2.3089, -2.1029, -2.5616, -1.4511, -1.7036, -1.8090, -2.1104],\n",
            "        [-2.1007, -2.3746, -2.1093, -1.3720, -1.7788, -1.8661, -2.4523],\n",
            "        [-2.0091, -2.1212, -2.3243, -1.8695, -1.7496, -1.6989, -1.9861],\n",
            "        [-2.5758, -2.0410, -2.8003, -1.2538, -1.8820, -1.5948, -2.3795],\n",
            "        [-2.0960, -2.0142, -2.6353, -1.0870, -2.3869, -2.0536, -2.1669],\n",
            "        [-2.6472, -1.9433, -2.4022, -1.0501, -2.2237, -1.7744, -2.6930],\n",
            "        [-1.7385, -2.4269, -2.3657, -0.9334, -2.3617, -2.5547, -2.5661],\n",
            "        [-2.7159, -1.8272, -2.0038, -1.0770, -2.4724, -1.8683, -2.8337],\n",
            "        [-2.1875, -2.0447, -2.0271, -1.3076, -2.0941, -1.9781, -2.3572],\n",
            "        [-1.9349, -2.1509, -2.1070, -1.4672, -1.9780, -2.0166, -2.1580],\n",
            "        [-1.8192, -2.3919, -2.2336, -1.4391, -1.8312, -1.9511, -2.3049],\n",
            "        [-1.8391, -1.6511, -2.2180, -2.1549, -2.3222, -1.7849, -1.8415],\n",
            "        [-1.8008, -2.4150, -2.2020, -0.8567, -2.5865, -2.5963, -2.8050],\n",
            "        [-1.9769, -2.3496, -2.4766, -1.9683, -1.3778, -1.8947, -1.9670],\n",
            "        [-1.9956, -2.2521, -2.1381, -1.3695, -1.9356, -1.9842, -2.2544],\n",
            "        [-2.2810, -2.2826, -2.6888, -1.3814, -1.5674, -1.8261, -2.2349],\n",
            "        [-2.2747, -2.1896, -2.7885, -1.5754, -1.6224, -1.6129, -2.1202],\n",
            "        [-2.2618, -2.1471, -2.3548, -0.8548, -2.4705, -2.1336, -2.8868],\n",
            "        [-2.2081, -2.3368, -2.5255, -2.3272, -1.2631, -1.8820, -1.7103],\n",
            "        [-2.4808, -1.9863, -2.8542, -1.3391, -1.9363, -1.5422, -2.2898],\n",
            "        [-1.8304, -1.7323, -2.2750, -2.0458, -2.2255, -1.7772, -1.8735],\n",
            "        [-1.8738, -1.8834, -2.2398, -1.5132, -2.3147, -1.8419, -2.2035],\n",
            "        [-2.3082, -2.2838, -2.6425, -1.9484, -1.2959, -1.8583, -1.8617],\n",
            "        [-2.6306, -2.1421, -3.1513, -1.2352, -1.7637, -1.5692, -2.3293],\n",
            "        [-1.8375, -1.7136, -2.2759, -2.0920, -2.2303, -1.7723, -1.8515],\n",
            "        [-2.3934, -2.2309, -2.9219, -1.3988, -1.6678, -1.6204, -2.1710],\n",
            "        [-1.8809, -2.5905, -2.0589, -1.3639, -1.7556, -1.9861, -2.5349],\n",
            "        [-2.5960, -2.1332, -3.2005, -1.2955, -1.7405, -1.5370, -2.2824],\n",
            "        [-1.7025, -2.4452, -2.4326, -1.6606, -1.7170, -1.8339, -2.1731],\n",
            "        [-1.8733, -2.5343, -2.3153, -1.1367, -1.8448, -2.1874, -2.5612],\n",
            "        [-2.2576, -2.0997, -2.6481, -1.5637, -1.7691, -1.5888, -2.1361],\n",
            "        [-1.8886, -1.7783, -1.5397, -2.1898, -2.5464, -2.7028, -1.5701],\n",
            "        [-1.8413, -1.7225, -2.2858, -2.0803, -2.2165, -1.7677, -1.8549],\n",
            "        [-2.5781, -2.1483, -3.1072, -1.2488, -1.7649, -1.5710, -2.3353],\n",
            "        [-1.6060, -2.0353, -2.1184, -1.8628, -2.0608, -2.0724, -1.9663],\n",
            "        [-2.6911, -2.0680, -3.0383, -1.1908, -1.8701, -1.5573, -2.4186],\n",
            "        [-1.4228, -2.3401, -1.9385, -1.2012, -2.8321, -2.5378, -2.5261],\n",
            "        [-2.2244, -1.9286, -2.3292, -0.9694, -2.6483, -2.0914, -2.5830],\n",
            "        [-1.6731, -2.7065, -2.5033, -2.1306, -1.2765, -2.2016, -1.8621],\n",
            "        [-1.5528, -2.4229, -2.2912, -1.5773, -1.8101, -2.2092, -2.1324],\n",
            "        [-1.8769, -1.6754, -2.3050, -2.0687, -2.2794, -1.7431, -1.8562],\n",
            "        [-1.9717, -2.4645, -2.2138, -2.1068, -1.3552, -2.0710, -1.8273],\n",
            "        [-2.1482, -2.3337, -2.4339, -1.7834, -1.4181, -1.9544, -1.9188],\n",
            "        [-1.6238, -2.5484, -1.8386, -2.2820, -1.4090, -2.4118, -2.0444],\n",
            "        [-2.5824, -2.1687, -3.0767, -1.1985, -1.7529, -1.6356, -2.3620],\n",
            "        [-2.3188, -2.1802, -2.6514, -2.0974, -1.4307, -1.6533, -1.8036],\n",
            "        [-1.8240, -2.1593, -2.0185, -1.8799, -1.8625, -1.8916, -2.0277],\n",
            "        [-2.5827, -1.8873, -2.2024, -1.1038, -2.3074, -1.8207, -2.6682],\n",
            "        [-2.0624, -1.8878, -2.1840, -1.0981, -2.6531, -2.0228, -2.6221]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9840ba522053>:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  f = F.log_softmax(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train BGAT and GAT models. "
      ],
      "metadata": {
        "id": "V6TJwO2IQA4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Myconv_BGAT()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(),  lr = 0.005)  # Define optimizer, an object for updating parameters\n",
        "#loss = loss + weight decay parameter * L2 norm of the weights, 1) To prevent overfitting 2) To keep the weights small and avoid exploding gradient\n",
        "\n",
        "def train_BGAT(Data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(Data[\"x\"], Data[\"edge_index\"])\n",
        "    train_loss_BGAT = criterion(out[Data[\"train_mask\"]], Data[\"y\"][Data[\"train_mask\"]])  # Compute the loss solely based on the training nodes.\n",
        "    train_loss_BGAT.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return train_loss_BGAT\n",
        "\n",
        "train_loss_BGAT_ = []\n",
        "for epoch in range(1, 101):\n",
        "  epoch_train_lossBGAT = []\n",
        "  train_loss_BGAT = train_BGAT(Data)\n",
        "  epoch_train_lossBGAT.append(train_loss_BGAT.item())\n",
        "  train_loss_BGAT_.append(sum(epoch_train_lossBGAT)/len(epoch_train_lossBGAT))\n",
        "  print(f'Train_BGAT:  {train_loss_BGAT:.3f}')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR1t_Jk4Ntag",
        "outputId": "b57a103e-dfda-48e5-b017-d1f308b6f86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_BGAT:  2.976\n",
            "Train_BGAT:  1.667\n",
            "Train_BGAT:  1.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-cedf2bd29c67>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  h = F.log_softmax(h)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_BGAT:  0.977\n",
            "Train_BGAT:  0.773\n",
            "Train_BGAT:  0.676\n",
            "Train_BGAT:  0.624\n",
            "Train_BGAT:  0.580\n",
            "Train_BGAT:  0.542\n",
            "Train_BGAT:  0.508\n",
            "Train_BGAT:  0.478\n",
            "Train_BGAT:  0.451\n",
            "Train_BGAT:  0.426\n",
            "Train_BGAT:  0.405\n",
            "Train_BGAT:  0.385\n",
            "Train_BGAT:  0.368\n",
            "Train_BGAT:  0.352\n",
            "Train_BGAT:  0.337\n",
            "Train_BGAT:  0.323\n",
            "Train_BGAT:  0.310\n",
            "Train_BGAT:  0.298\n",
            "Train_BGAT:  0.287\n",
            "Train_BGAT:  0.277\n",
            "Train_BGAT:  0.267\n",
            "Train_BGAT:  0.258\n",
            "Train_BGAT:  0.250\n",
            "Train_BGAT:  0.241\n",
            "Train_BGAT:  0.234\n",
            "Train_BGAT:  0.227\n",
            "Train_BGAT:  0.221\n",
            "Train_BGAT:  0.215\n",
            "Train_BGAT:  0.209\n",
            "Train_BGAT:  0.204\n",
            "Train_BGAT:  0.199\n",
            "Train_BGAT:  0.194\n",
            "Train_BGAT:  0.189\n",
            "Train_BGAT:  0.185\n",
            "Train_BGAT:  0.181\n",
            "Train_BGAT:  0.177\n",
            "Train_BGAT:  0.172\n",
            "Train_BGAT:  0.168\n",
            "Train_BGAT:  0.165\n",
            "Train_BGAT:  0.161\n",
            "Train_BGAT:  0.158\n",
            "Train_BGAT:  0.155\n",
            "Train_BGAT:  0.151\n",
            "Train_BGAT:  0.148\n",
            "Train_BGAT:  0.145\n",
            "Train_BGAT:  0.143\n",
            "Train_BGAT:  0.140\n",
            "Train_BGAT:  0.137\n",
            "Train_BGAT:  0.135\n",
            "Train_BGAT:  0.132\n",
            "Train_BGAT:  0.130\n",
            "Train_BGAT:  0.127\n",
            "Train_BGAT:  0.125\n",
            "Train_BGAT:  0.123\n",
            "Train_BGAT:  0.121\n",
            "Train_BGAT:  0.118\n",
            "Train_BGAT:  0.116\n",
            "Train_BGAT:  0.114\n",
            "Train_BGAT:  0.112\n",
            "Train_BGAT:  0.110\n",
            "Train_BGAT:  0.108\n",
            "Train_BGAT:  0.106\n",
            "Train_BGAT:  0.104\n",
            "Train_BGAT:  0.103\n",
            "Train_BGAT:  0.101\n",
            "Train_BGAT:  0.099\n",
            "Train_BGAT:  0.097\n",
            "Train_BGAT:  0.096\n",
            "Train_BGAT:  0.094\n",
            "Train_BGAT:  0.093\n",
            "Train_BGAT:  0.091\n",
            "Train_BGAT:  0.090\n",
            "Train_BGAT:  0.088\n",
            "Train_BGAT:  0.087\n",
            "Train_BGAT:  0.085\n",
            "Train_BGAT:  0.084\n",
            "Train_BGAT:  0.083\n",
            "Train_BGAT:  0.081\n",
            "Train_BGAT:  0.080\n",
            "Train_BGAT:  0.079\n",
            "Train_BGAT:  0.078\n",
            "Train_BGAT:  0.076\n",
            "Train_BGAT:  0.075\n",
            "Train_BGAT:  0.074\n",
            "Train_BGAT:  0.073\n",
            "Train_BGAT:  0.071\n",
            "Train_BGAT:  0.070\n",
            "Train_BGAT:  0.069\n",
            "Train_BGAT:  0.068\n",
            "Train_BGAT:  0.067\n",
            "Train_BGAT:  0.066\n",
            "Train_BGAT:  0.065\n",
            "Train_BGAT:  0.064\n",
            "Train_BGAT:  0.063\n",
            "Train_BGAT:  0.062\n",
            "Train_BGAT:  0.061\n",
            "Train_BGAT:  0.060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Myconv_GAT()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(),  lr = 0.0005)  # Define optimizer, an object for updating parameters\n",
        "#loss = loss + weight decay parameter * L2 norm of the weights, 1) To prevent overfitting 2) To keep the weights small and avoid exploding gradient\n",
        "\n",
        "def train_GAT(Data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(Data[\"x\"], Data[\"edge_index\"])\n",
        "    train_loss_GAT = criterion(out[Data[\"train_mask\"]], Data[\"y\"][Data[\"train_mask\"]])  # Compute the loss solely based on the training nodes.\n",
        "    train_loss_GAT.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return train_loss_GAT\n",
        "\n",
        "train_loss_GAT_ = []\n",
        "for epoch in range(1, 101):\n",
        "  epoch_train_lossGAT = []\n",
        "  train_loss_GAT = train_GAT(Data)\n",
        "  epoch_train_lossGAT.append(train_loss_GAT.item())\n",
        "  train_loss_GAT_.append(sum(epoch_train_lossGAT)/len(epoch_train_lossGAT))\n",
        "  print(f'Train_GAT:  {train_loss_GAT:.3f}')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGse15hlNvCq",
        "outputId": "d45cddba-92f6-4e6d-d6f5-019c7c11c8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_GAT:  2.105\n",
            "Train_GAT:  1.922\n",
            "Train_GAT:  1.786\n",
            "Train_GAT:  1.701\n",
            "Train_GAT:  1.632\n",
            "Train_GAT:  1.566\n",
            "Train_GAT:  1.501\n",
            "Train_GAT:  1.447\n",
            "Train_GAT:  1.397\n",
            "Train_GAT:  1.350\n",
            "Train_GAT:  1.307\n",
            "Train_GAT:  1.264\n",
            "Train_GAT:  1.223\n",
            "Train_GAT:  1.185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-9840ba522053>:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  f = F.log_softmax(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_GAT:  1.151\n",
            "Train_GAT:  1.122\n",
            "Train_GAT:  1.097\n",
            "Train_GAT:  1.074\n",
            "Train_GAT:  1.052\n",
            "Train_GAT:  1.031\n",
            "Train_GAT:  1.010\n",
            "Train_GAT:  0.989\n",
            "Train_GAT:  0.969\n",
            "Train_GAT:  0.949\n",
            "Train_GAT:  0.930\n",
            "Train_GAT:  0.912\n",
            "Train_GAT:  0.895\n",
            "Train_GAT:  0.880\n",
            "Train_GAT:  0.867\n",
            "Train_GAT:  0.855\n",
            "Train_GAT:  0.843\n",
            "Train_GAT:  0.831\n",
            "Train_GAT:  0.819\n",
            "Train_GAT:  0.808\n",
            "Train_GAT:  0.797\n",
            "Train_GAT:  0.787\n",
            "Train_GAT:  0.777\n",
            "Train_GAT:  0.769\n",
            "Train_GAT:  0.760\n",
            "Train_GAT:  0.752\n",
            "Train_GAT:  0.744\n",
            "Train_GAT:  0.737\n",
            "Train_GAT:  0.730\n",
            "Train_GAT:  0.724\n",
            "Train_GAT:  0.718\n",
            "Train_GAT:  0.712\n",
            "Train_GAT:  0.706\n",
            "Train_GAT:  0.700\n",
            "Train_GAT:  0.695\n",
            "Train_GAT:  0.690\n",
            "Train_GAT:  0.685\n",
            "Train_GAT:  0.680\n",
            "Train_GAT:  0.675\n",
            "Train_GAT:  0.670\n",
            "Train_GAT:  0.665\n",
            "Train_GAT:  0.660\n",
            "Train_GAT:  0.656\n",
            "Train_GAT:  0.651\n",
            "Train_GAT:  0.647\n",
            "Train_GAT:  0.643\n",
            "Train_GAT:  0.639\n",
            "Train_GAT:  0.635\n",
            "Train_GAT:  0.631\n",
            "Train_GAT:  0.627\n",
            "Train_GAT:  0.623\n",
            "Train_GAT:  0.619\n",
            "Train_GAT:  0.616\n",
            "Train_GAT:  0.612\n",
            "Train_GAT:  0.608\n",
            "Train_GAT:  0.605\n",
            "Train_GAT:  0.601\n",
            "Train_GAT:  0.598\n",
            "Train_GAT:  0.594\n",
            "Train_GAT:  0.591\n",
            "Train_GAT:  0.588\n",
            "Train_GAT:  0.585\n",
            "Train_GAT:  0.581\n",
            "Train_GAT:  0.578\n",
            "Train_GAT:  0.575\n",
            "Train_GAT:  0.572\n",
            "Train_GAT:  0.569\n",
            "Train_GAT:  0.566\n",
            "Train_GAT:  0.563\n",
            "Train_GAT:  0.559\n",
            "Train_GAT:  0.557\n",
            "Train_GAT:  0.554\n",
            "Train_GAT:  0.551\n",
            "Train_GAT:  0.548\n",
            "Train_GAT:  0.545\n",
            "Train_GAT:  0.542\n",
            "Train_GAT:  0.539\n",
            "Train_GAT:  0.536\n",
            "Train_GAT:  0.534\n",
            "Train_GAT:  0.531\n",
            "Train_GAT:  0.528\n",
            "Train_GAT:  0.526\n",
            "Train_GAT:  0.523\n",
            "Train_GAT:  0.520\n",
            "Train_GAT:  0.518\n",
            "Train_GAT:  0.515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize train_loss for both models. "
      ],
      "metadata": {
        "id": "3JYZgu6cQHm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss_BGAT_, \"m\")\n",
        "plt.plot(train_loss_GAT_, \"c\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Train_BGAT','Train_GAT'])\n",
        "plt.title('Second scenario, Train_BGAT vs Train_GAT ')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "C9tIhc65Y2aa",
        "outputId": "7b5fd0a7-32ee-489a-cb54-4fe912fbc465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}