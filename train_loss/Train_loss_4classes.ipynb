{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount to gdrive to read data, adjacency matrix, p_link and labels. "
      ],
      "metadata": {
        "id": "dWu3Z-XWQsh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1Goe68YyFV8",
        "outputId": "1f976225-f322-4348-e971-228bf78a4ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pandas to read csv files from gdrive. Install torch and numpy which are packages that are needed durind coding."
      ],
      "metadata": {
        "id": "zcOnvP1RQvXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30N90I8wyuLO",
        "outputId": "e546e44e-340d-4354-def2-87d62ca153e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/gdrive/MyDrive/4classes.sim/data.pt\"\n",
        "Data = torch.load(path)\n",
        "Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1ef8v-Qywv6",
        "outputId": "137ef95e-bc3c-46f5-b0d8-d23924e6fd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': tensor([[-0.4530,  0.5955, -1.1069,  ...,  0.5985,  0.1790,  0.2985],\n",
              "         [ 0.2027,  0.6537,  0.6290,  ..., -0.1876, -1.2782,  0.5982],\n",
              "         [-0.2713,  0.4418,  1.5845,  ..., -1.1430, -2.2012,  0.7388],\n",
              "         ...,\n",
              "         [ 0.5176, -1.6871, -1.4386,  ..., -0.2084,  1.3068, -0.1615],\n",
              "         [ 0.0700, -1.4413, -0.2656,  ..., -0.3264,  1.0155,  0.4870],\n",
              "         [-0.2865,  1.4095,  1.7289,  ..., -0.3547, -1.7226,  0.7805]]),\n",
              " 'edge_index': tensor([[ 0,  0,  1,  1,  1,  2,  2,  2,  2,  4,  4,  4,  4,  5,  5,  6,  6,  6,\n",
              "           6,  6,  6,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9, 10, 10,\n",
              "          11, 11, 11, 11, 11, 12, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
              "          14, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 20, 20, 21, 21, 21, 21,\n",
              "          21, 21, 22, 23, 24, 24, 24, 24, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26,\n",
              "          27, 28, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 32, 32,\n",
              "          33, 34, 34, 34, 34, 34, 34, 35, 36, 37, 37, 37, 37, 37, 38, 38, 39, 39],\n",
              "         [17, 33, 18, 26, 37,  5, 10, 23, 25, 14, 21, 29, 30,  2, 16,  8,  9, 14,\n",
              "          29, 30, 34, 36,  6,  9, 14, 29, 30, 34,  6,  8, 21, 29, 30, 34,  2, 11,\n",
              "          10, 18, 26, 27, 37, 25, 20, 22,  4,  6,  8, 19, 21, 24, 29, 30, 34, 37,\n",
              "          39,  5,  0, 20, 39,  1, 11, 26, 32, 14, 26, 37, 13, 17,  4,  9, 14, 24,\n",
              "          29, 34, 13,  2, 14, 21, 30, 34,  2, 12,  1, 11, 18, 19, 28, 35, 37, 38,\n",
              "          11, 26,  4,  6,  8,  9, 14, 21, 30,  4,  6,  8,  9, 14, 24, 29, 18, 38,\n",
              "           0,  6,  8,  9, 14, 21, 24, 26,  7,  1, 11, 14, 19, 26, 26, 32, 14, 17]]),\n",
              " 'y': tensor([0, 2, 3, 0, 1, 3, 1, 3, 1, 1, 3, 2, 3, 0, 1, 3, 3, 0, 2, 2, 0, 1, 0, 3,\n",
              "         1, 3, 2, 2, 2, 1, 1, 0, 0, 0, 1, 2, 3, 2, 2, 0]),\n",
              " 'train_mask': tensor([ True, False, False, False, False,  True, False, False, False,  True,\n",
              "         False, False,  True, False,  True, False, False, False,  True, False,\n",
              "         False, False,  True, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False]),\n",
              " 'val_mask': tensor([False, False, False, False, False, False, False,  True,  True, False,\n",
              "         False,  True, False, False, False,  True, False, False, False, False,\n",
              "          True,  True, False,  True, False, False, False, False,  True, False,\n",
              "         False,  True, False, False, False, False, False, False,  True, False]),\n",
              " 'test_mask': tensor([False,  True,  True,  True,  True, False,  True, False, False, False,\n",
              "          True, False, False,  True, False, False,  True,  True, False,  True,\n",
              "         False, False, False, False,  True,  True,  True, False, False,  True,\n",
              "          True, False,  True,  True,  True,  True,  True,  True, False,  True])}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/gdrive/MyDrive/4classes.sim/p_link.pt\"\n",
        "p_link = torch.load(path)\n",
        "print(p_link)\n",
        "print(p_link.size())\n",
        "print(p_link.dtype)\n",
        "print(p_link.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Wx_7tjy1Sy",
        "outputId": "2b6389f3-cc9e-41f8-b41c-fe5daf4ce000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0900, 0.0000],\n",
            "        [0.0000, 1.0000, 0.2000,  ..., 0.6900, 0.0200, 0.2100],\n",
            "        [0.0000, 0.2000, 1.0000,  ..., 0.0300, 0.0300, 0.0100],\n",
            "        ...,\n",
            "        [0.0200, 0.6900, 0.0300,  ..., 1.0000, 0.1600, 0.5800],\n",
            "        [0.0900, 0.0200, 0.0300,  ..., 0.1600, 1.0000, 0.2000],\n",
            "        [0.0000, 0.2100, 0.0100,  ..., 0.5800, 0.2000, 1.0000]])\n",
            "torch.Size([40, 40])\n",
            "torch.float32\n",
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install torch_geometic."
      ],
      "metadata": {
        "id": "kVqXNqw3QzjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9SsnbJ_z9UV",
        "outputId": "f9bf5905-672e-4918-aa2b-c5cea627c89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 27.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 28.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create BGAT and GAT layers. "
      ],
      "metadata": {
        "id": "Bboe7xS-Q3Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import MessagePassing\n",
        "import torch_geometric.nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch_geometric.nn import GATConv"
      ],
      "metadata": {
        "id": "NfnXrtIEy8wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BGATConv(MessagePassing):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(node_dim = 0, **kwargs)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index, _ = add_self_loops(Data[\"edge_index\"], num_nodes = Data[\"x\"].size(0))\n",
        "        x = self.lin(x)\n",
        "        atten = p_link\n",
        "        return self.propagate(edge_index, x=x, atten = atten)\n",
        "\n",
        "    def message(self, x_j, atten, edge_index_i, edge_index_j):\n",
        "        return atten[edge_index_i, edge_index_j].reshape(-1,1) * x_j\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)"
      ],
      "metadata": {
        "id": "hAk8kdxQy_Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Myconv_BGAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(2)\n",
        "        self.conv1 = BGATConv(in_channels = 500, out_channels = 50)\n",
        "        self.conv2 =  BGATConv(50, 10)\n",
        "        self.conv3 =  BGATConv(10, 4)\n",
        "        \n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = F.torch.tanh(h)\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = F.torch.tanh(h)\n",
        "        h = self.conv3(h, edge_index)# Final GNN embedding space.\n",
        "        h = F.log_softmax(h)\n",
        "        return h\n",
        "        \n",
        "model_BGAT = Myconv_BGAT()\n",
        "print(model_BGAT)\n",
        "print(model_BGAT.forward(Data[\"x\"], Data[\"edge_index\"]))"
      ],
      "metadata": {
        "id": "GWNxvyEnzCfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7432a7-63f9-40eb-8628-b1cc71ff9e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Myconv_BGAT(\n",
            "  (conv1): BGATConv(500, 50)\n",
            "  (conv2): BGATConv(50, 10)\n",
            "  (conv3): BGATConv(10, 4)\n",
            ")\n",
            "tensor([[ -2.8707,  -3.4974,  -0.1526,  -2.9075],\n",
            "        [ -1.0652,  -1.9093,  -1.5955,  -1.1896],\n",
            "        [ -2.1635,  -3.3861,  -0.3989,  -1.7140],\n",
            "        [ -1.3228,  -1.4786,  -1.5151,  -1.2521],\n",
            "        [ -2.0014,  -0.6609,  -1.0544,  -9.5835],\n",
            "        [ -2.0046,  -2.8759,  -0.6233,  -1.2992],\n",
            "        [ -2.8296,  -0.2859,  -1.6626, -12.7543],\n",
            "        [ -1.1789,  -1.5411,  -1.6864,  -1.2275],\n",
            "        [ -2.8913,  -0.2720,  -1.7003, -13.0820],\n",
            "        [ -2.4686,  -0.4631,  -1.2520, -11.6449],\n",
            "        [ -1.4750,  -1.7880,  -0.9600,  -1.5094],\n",
            "        [ -0.9170,  -1.2654,  -1.9920,  -1.7053],\n",
            "        [ -1.4784,  -2.0022,  -0.9031,  -1.4624],\n",
            "        [ -2.7180,  -2.9736,  -0.2210,  -2.5118],\n",
            "        [ -2.8403,  -0.2415,  -1.8570, -14.6291],\n",
            "        [ -1.3367,  -1.5211,  -1.3278,  -1.3714],\n",
            "        [ -1.6430,  -2.3355,  -0.8739,  -1.2291],\n",
            "        [ -3.4319,  -4.1346,  -0.0958,  -3.1470],\n",
            "        [ -1.1831,  -2.2075,  -1.4006,  -1.0869],\n",
            "        [ -0.9423,  -1.0722,  -1.9305,  -2.0961],\n",
            "        [ -2.9920,  -3.2575,  -0.1752,  -2.6303],\n",
            "        [ -2.0790,  -0.8455,  -0.8084, -10.1432],\n",
            "        [ -2.1252,  -2.2306,  -0.4491,  -2.0028],\n",
            "        [ -1.6865,  -2.1122,  -0.8501,  -1.3223],\n",
            "        [ -1.8387,  -0.7721,  -0.9707,  -9.2265],\n",
            "        [ -1.8138,  -2.5164,  -0.6264,  -1.5064],\n",
            "        [ -0.2137,  -2.4092,  -4.4890,  -2.3934],\n",
            "        [ -1.0846,  -0.9907,  -1.9579,  -1.9005],\n",
            "        [ -1.2152,  -1.7112,  -1.5653,  -1.1594],\n",
            "        [ -2.8401,  -0.3090,  -1.5732, -13.0521],\n",
            "        [ -3.2143,  -0.2485,  -1.7157, -14.8221],\n",
            "        [ -1.3428,  -1.4406,  -1.6031,  -1.2012],\n",
            "        [ -0.9815,  -2.1195,  -1.9237,  -1.0241],\n",
            "        [ -2.0548,  -2.5299,  -0.3743,  -2.2593],\n",
            "        [ -2.6582,  -0.4244,  -1.2882, -12.5098],\n",
            "        [ -1.2376,  -1.9160,  -1.3145,  -1.2238],\n",
            "        [ -1.1789,  -1.5411,  -1.6864,  -1.2275],\n",
            "        [ -0.7774,  -1.2649,  -2.2945,  -1.8493],\n",
            "        [ -1.1652,  -2.4666,  -1.7944,  -0.8278],\n",
            "        [ -2.0863,  -2.3084,  -0.4300,  -2.0723]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-821b25de8f36>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  h = F.log_softmax(h)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Myconv_GAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(2)\n",
        "        self.conv1 = GATConv(in_channels = 500, out_channels = 8)\n",
        "        self.conv2 =  GATConv(8,4)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        f = self.conv1(x, edge_index)\n",
        "        f = F.torch.tanh(f)\n",
        "        f = self.conv2(f, edge_index)# Final GNN embedding space.\n",
        "        f = F.log_softmax(f)\n",
        "        return f\n",
        "        \n",
        "\n",
        "model_GAT = Myconv_GAT()\n",
        "print(model_GAT)\n",
        "print(model_GAT.forward(Data[\"x\"], Data[\"edge_index\"]))"
      ],
      "metadata": {
        "id": "Q9jEr1XnzOW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94481122-80b4-4c4c-9e1a-ab4f9a9a845d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Myconv_GAT(\n",
            "  (conv1): GATConv(500, 8, heads=1)\n",
            "  (conv2): GATConv(8, 4, heads=1)\n",
            ")\n",
            "tensor([[-1.4782, -3.2267, -1.6484, -0.6164],\n",
            "        [-2.0163, -2.9077, -1.6430, -0.4799],\n",
            "        [-0.9537, -1.8489, -2.1233, -1.0858],\n",
            "        [-2.5058, -2.3223, -0.3816, -1.9836],\n",
            "        [-1.0091, -2.2511, -1.4071, -1.2542],\n",
            "        [-1.2715, -1.9648, -2.0565, -0.7952],\n",
            "        [-0.9781, -2.0685, -1.6489, -1.1863],\n",
            "        [-1.2810, -0.7253, -2.0576, -2.2048],\n",
            "        [-0.9781, -2.0685, -1.6489, -1.1863],\n",
            "        [-1.0504, -2.1256, -1.5886, -1.1190],\n",
            "        [-1.4075, -1.5120, -1.7793, -1.0051],\n",
            "        [-1.8909, -2.5580, -1.5674, -0.5744],\n",
            "        [-0.4946, -1.8777, -2.1263, -2.1373],\n",
            "        [-1.4524, -2.7654, -1.5383, -0.7168],\n",
            "        [-1.1531, -2.7234, -1.3636, -1.0135],\n",
            "        [-1.4762, -1.8212, -1.0878, -1.2994],\n",
            "        [-1.2769, -2.0969, -2.1223, -0.7371],\n",
            "        [-1.4118, -3.4797, -1.4304, -0.7210],\n",
            "        [-2.0807, -2.8264, -1.6534, -0.4707],\n",
            "        [-1.5796, -2.4318, -1.4956, -0.7299],\n",
            "        [-1.4831, -3.3766, -1.4775, -0.6720],\n",
            "        [-1.0161, -2.1776, -1.3977, -1.2819],\n",
            "        [-1.2698, -1.0947, -2.0855, -1.3463],\n",
            "        [-1.1697, -1.6640, -2.0356, -0.9954],\n",
            "        [-1.1055, -2.0337, -1.4079, -1.2260],\n",
            "        [-0.6846, -1.7376, -1.9916, -1.6967],\n",
            "        [-2.0622, -2.7837, -1.7222, -0.4583],\n",
            "        [-2.0313, -1.1905, -1.3081, -1.2226],\n",
            "        [-2.1968, -2.8560, -1.8019, -0.4059],\n",
            "        [-0.9979, -2.2088, -1.4881, -1.2184],\n",
            "        [-0.9643, -2.1317, -1.4674, -1.3108],\n",
            "        [-3.3169, -1.8301, -0.9734, -0.8544],\n",
            "        [-2.0530, -2.6790, -1.6247, -0.5008],\n",
            "        [-1.5837, -2.4563, -2.1402, -0.5252],\n",
            "        [-1.0640, -2.0765, -1.4935, -1.1876],\n",
            "        [-2.2714, -2.9443, -1.8941, -0.3657],\n",
            "        [-1.2810, -0.7253, -2.0576, -2.2048],\n",
            "        [-1.7555, -2.5869, -1.5488, -0.6173],\n",
            "        [-1.9904, -2.5437, -1.5481, -0.5584],\n",
            "        [-1.3121, -3.5187, -1.3645, -0.8084]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-3c123f49bc3d>:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  f = F.log_softmax(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train BGAT and GAT models. "
      ],
      "metadata": {
        "id": "sSf-BXhKQ8DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Myconv_BGAT()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(),  lr = 0.0005)  # Define optimizer, an object for updating parameters\n",
        "#loss = loss + weight decay parameter * L2 norm of the weights, 1) To prevent overfitting 2) To keep the weights small and avoid exploding gradient\n",
        "\n",
        "def train_BGAT(Data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(Data[\"x\"], Data[\"edge_index\"])\n",
        "    train_loss_BGAT = criterion(out[Data[\"train_mask\"]], Data[\"y\"][Data[\"train_mask\"]])  # Compute the loss solely based on the training nodes.\n",
        "    train_loss_BGAT.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return train_loss_BGAT\n",
        "\n",
        "train_loss_BGAT_ = []\n",
        "for epoch in range(1, 201):\n",
        "  epoch_train_lossBGAT = []\n",
        "  train_loss_BGAT = train_BGAT(Data)\n",
        "  epoch_train_lossBGAT.append(train_loss_BGAT.item())\n",
        "  train_loss_BGAT_.append(sum(epoch_train_lossBGAT)/len(epoch_train_lossBGAT))\n",
        "  print(f'Train_BGAT:  {train_loss_BGAT:.3f}')  "
      ],
      "metadata": {
        "id": "ipX-THD7zZlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e220918-ae8d-47fb-d63b-7e6047c94c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-821b25de8f36>:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  h = F.log_softmax(h)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_BGAT:  1.478\n",
            "Train_BGAT:  0.957\n",
            "Train_BGAT:  0.624\n",
            "Train_BGAT:  0.415\n",
            "Train_BGAT:  0.288\n",
            "Train_BGAT:  0.214\n",
            "Train_BGAT:  0.169\n",
            "Train_BGAT:  0.141\n",
            "Train_BGAT:  0.121\n",
            "Train_BGAT:  0.107\n",
            "Train_BGAT:  0.096\n",
            "Train_BGAT:  0.087\n",
            "Train_BGAT:  0.080\n",
            "Train_BGAT:  0.074\n",
            "Train_BGAT:  0.069\n",
            "Train_BGAT:  0.064\n",
            "Train_BGAT:  0.060\n",
            "Train_BGAT:  0.057\n",
            "Train_BGAT:  0.054\n",
            "Train_BGAT:  0.051\n",
            "Train_BGAT:  0.049\n",
            "Train_BGAT:  0.047\n",
            "Train_BGAT:  0.045\n",
            "Train_BGAT:  0.044\n",
            "Train_BGAT:  0.043\n",
            "Train_BGAT:  0.042\n",
            "Train_BGAT:  0.041\n",
            "Train_BGAT:  0.040\n",
            "Train_BGAT:  0.039\n",
            "Train_BGAT:  0.038\n",
            "Train_BGAT:  0.038\n",
            "Train_BGAT:  0.037\n",
            "Train_BGAT:  0.037\n",
            "Train_BGAT:  0.036\n",
            "Train_BGAT:  0.036\n",
            "Train_BGAT:  0.035\n",
            "Train_BGAT:  0.035\n",
            "Train_BGAT:  0.035\n",
            "Train_BGAT:  0.034\n",
            "Train_BGAT:  0.034\n",
            "Train_BGAT:  0.034\n",
            "Train_BGAT:  0.033\n",
            "Train_BGAT:  0.033\n",
            "Train_BGAT:  0.033\n",
            "Train_BGAT:  0.032\n",
            "Train_BGAT:  0.032\n",
            "Train_BGAT:  0.032\n",
            "Train_BGAT:  0.032\n",
            "Train_BGAT:  0.032\n",
            "Train_BGAT:  0.031\n",
            "Train_BGAT:  0.031\n",
            "Train_BGAT:  0.031\n",
            "Train_BGAT:  0.031\n",
            "Train_BGAT:  0.030\n",
            "Train_BGAT:  0.030\n",
            "Train_BGAT:  0.030\n",
            "Train_BGAT:  0.030\n",
            "Train_BGAT:  0.030\n",
            "Train_BGAT:  0.030\n",
            "Train_BGAT:  0.029\n",
            "Train_BGAT:  0.029\n",
            "Train_BGAT:  0.029\n",
            "Train_BGAT:  0.029\n",
            "Train_BGAT:  0.029\n",
            "Train_BGAT:  0.029\n",
            "Train_BGAT:  0.028\n",
            "Train_BGAT:  0.028\n",
            "Train_BGAT:  0.028\n",
            "Train_BGAT:  0.028\n",
            "Train_BGAT:  0.028\n",
            "Train_BGAT:  0.028\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.027\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.026\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.025\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.024\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.023\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.022\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.021\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.020\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.019\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.018\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.017\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.016\n",
            "Train_BGAT:  0.015\n",
            "Train_BGAT:  0.015\n",
            "Train_BGAT:  0.015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Myconv_GAT()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(),  lr = 0.0005)  # Define optimizer, an object for updating parameters\n",
        "#loss = loss + weight decay parameter * L2 norm of the weights, 1) To prevent overfitting 2) To keep the weights small and avoid exploding gradient\n",
        "\n",
        "def train_GAT(Data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(Data[\"x\"], Data[\"edge_index\"])\n",
        "    train_loss_GAT = criterion(out[Data[\"train_mask\"]], Data[\"y\"][Data[\"train_mask\"]])  # Compute the loss solely based on the training nodes.\n",
        "    train_loss_GAT.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return train_loss_GAT\n",
        "\n",
        "train_loss_GAT_ = []\n",
        "for epoch in range(1, 201):\n",
        "  epoch_train_lossGAT = []\n",
        "  train_loss_GAT = train_GAT(Data)\n",
        "  epoch_train_lossGAT.append(train_loss_GAT.item())\n",
        "  train_loss_GAT_.append(sum(epoch_train_lossGAT)/len(epoch_train_lossGAT))\n",
        "  print(f'Train_GAT:  {train_loss_GAT:.3f}')    "
      ],
      "metadata": {
        "id": "4EXm7O0nzg3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b64fb54-25d7-417d-e251-7c5dea48b285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_GAT:  1.686\n",
            "Train_GAT:  1.583\n",
            "Train_GAT:  1.483\n",
            "Train_GAT:  1.389\n",
            "Train_GAT:  1.304\n",
            "Train_GAT:  1.229\n",
            "Train_GAT:  1.158\n",
            "Train_GAT:  1.088\n",
            "Train_GAT:  1.023\n",
            "Train_GAT:  0.961\n",
            "Train_GAT:  0.902\n",
            "Train_GAT:  0.846\n",
            "Train_GAT:  0.793\n",
            "Train_GAT:  0.745\n",
            "Train_GAT:  0.700\n",
            "Train_GAT:  0.658\n",
            "Train_GAT:  0.620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-3c123f49bc3d>:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  f = F.log_softmax(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_GAT:  0.584\n",
            "Train_GAT:  0.551\n",
            "Train_GAT:  0.521\n",
            "Train_GAT:  0.492\n",
            "Train_GAT:  0.466\n",
            "Train_GAT:  0.442\n",
            "Train_GAT:  0.419\n",
            "Train_GAT:  0.399\n",
            "Train_GAT:  0.380\n",
            "Train_GAT:  0.364\n",
            "Train_GAT:  0.349\n",
            "Train_GAT:  0.335\n",
            "Train_GAT:  0.322\n",
            "Train_GAT:  0.311\n",
            "Train_GAT:  0.301\n",
            "Train_GAT:  0.291\n",
            "Train_GAT:  0.283\n",
            "Train_GAT:  0.276\n",
            "Train_GAT:  0.269\n",
            "Train_GAT:  0.262\n",
            "Train_GAT:  0.257\n",
            "Train_GAT:  0.251\n",
            "Train_GAT:  0.247\n",
            "Train_GAT:  0.242\n",
            "Train_GAT:  0.237\n",
            "Train_GAT:  0.233\n",
            "Train_GAT:  0.229\n",
            "Train_GAT:  0.225\n",
            "Train_GAT:  0.222\n",
            "Train_GAT:  0.218\n",
            "Train_GAT:  0.215\n",
            "Train_GAT:  0.211\n",
            "Train_GAT:  0.208\n",
            "Train_GAT:  0.205\n",
            "Train_GAT:  0.202\n",
            "Train_GAT:  0.199\n",
            "Train_GAT:  0.196\n",
            "Train_GAT:  0.193\n",
            "Train_GAT:  0.190\n",
            "Train_GAT:  0.188\n",
            "Train_GAT:  0.185\n",
            "Train_GAT:  0.183\n",
            "Train_GAT:  0.181\n",
            "Train_GAT:  0.178\n",
            "Train_GAT:  0.176\n",
            "Train_GAT:  0.174\n",
            "Train_GAT:  0.172\n",
            "Train_GAT:  0.171\n",
            "Train_GAT:  0.169\n",
            "Train_GAT:  0.167\n",
            "Train_GAT:  0.166\n",
            "Train_GAT:  0.164\n",
            "Train_GAT:  0.163\n",
            "Train_GAT:  0.162\n",
            "Train_GAT:  0.161\n",
            "Train_GAT:  0.159\n",
            "Train_GAT:  0.158\n",
            "Train_GAT:  0.157\n",
            "Train_GAT:  0.156\n",
            "Train_GAT:  0.155\n",
            "Train_GAT:  0.154\n",
            "Train_GAT:  0.153\n",
            "Train_GAT:  0.152\n",
            "Train_GAT:  0.151\n",
            "Train_GAT:  0.150\n",
            "Train_GAT:  0.149\n",
            "Train_GAT:  0.148\n",
            "Train_GAT:  0.148\n",
            "Train_GAT:  0.147\n",
            "Train_GAT:  0.146\n",
            "Train_GAT:  0.145\n",
            "Train_GAT:  0.144\n",
            "Train_GAT:  0.143\n",
            "Train_GAT:  0.143\n",
            "Train_GAT:  0.142\n",
            "Train_GAT:  0.141\n",
            "Train_GAT:  0.141\n",
            "Train_GAT:  0.140\n",
            "Train_GAT:  0.139\n",
            "Train_GAT:  0.138\n",
            "Train_GAT:  0.138\n",
            "Train_GAT:  0.137\n",
            "Train_GAT:  0.136\n",
            "Train_GAT:  0.136\n",
            "Train_GAT:  0.135\n",
            "Train_GAT:  0.134\n",
            "Train_GAT:  0.134\n",
            "Train_GAT:  0.133\n",
            "Train_GAT:  0.132\n",
            "Train_GAT:  0.132\n",
            "Train_GAT:  0.131\n",
            "Train_GAT:  0.131\n",
            "Train_GAT:  0.130\n",
            "Train_GAT:  0.129\n",
            "Train_GAT:  0.129\n",
            "Train_GAT:  0.128\n",
            "Train_GAT:  0.128\n",
            "Train_GAT:  0.127\n",
            "Train_GAT:  0.126\n",
            "Train_GAT:  0.126\n",
            "Train_GAT:  0.125\n",
            "Train_GAT:  0.125\n",
            "Train_GAT:  0.124\n",
            "Train_GAT:  0.124\n",
            "Train_GAT:  0.123\n",
            "Train_GAT:  0.123\n",
            "Train_GAT:  0.122\n",
            "Train_GAT:  0.121\n",
            "Train_GAT:  0.121\n",
            "Train_GAT:  0.120\n",
            "Train_GAT:  0.120\n",
            "Train_GAT:  0.119\n",
            "Train_GAT:  0.119\n",
            "Train_GAT:  0.118\n",
            "Train_GAT:  0.118\n",
            "Train_GAT:  0.117\n",
            "Train_GAT:  0.117\n",
            "Train_GAT:  0.116\n",
            "Train_GAT:  0.116\n",
            "Train_GAT:  0.115\n",
            "Train_GAT:  0.115\n",
            "Train_GAT:  0.114\n",
            "Train_GAT:  0.114\n",
            "Train_GAT:  0.113\n",
            "Train_GAT:  0.113\n",
            "Train_GAT:  0.112\n",
            "Train_GAT:  0.112\n",
            "Train_GAT:  0.111\n",
            "Train_GAT:  0.111\n",
            "Train_GAT:  0.110\n",
            "Train_GAT:  0.110\n",
            "Train_GAT:  0.109\n",
            "Train_GAT:  0.109\n",
            "Train_GAT:  0.108\n",
            "Train_GAT:  0.108\n",
            "Train_GAT:  0.108\n",
            "Train_GAT:  0.107\n",
            "Train_GAT:  0.107\n",
            "Train_GAT:  0.106\n",
            "Train_GAT:  0.106\n",
            "Train_GAT:  0.105\n",
            "Train_GAT:  0.105\n",
            "Train_GAT:  0.104\n",
            "Train_GAT:  0.104\n",
            "Train_GAT:  0.103\n",
            "Train_GAT:  0.102\n",
            "Train_GAT:  0.102\n",
            "Train_GAT:  0.101\n",
            "Train_GAT:  0.101\n",
            "Train_GAT:  0.100\n",
            "Train_GAT:  0.100\n",
            "Train_GAT:  0.099\n",
            "Train_GAT:  0.099\n",
            "Train_GAT:  0.099\n",
            "Train_GAT:  0.098\n",
            "Train_GAT:  0.098\n",
            "Train_GAT:  0.097\n",
            "Train_GAT:  0.097\n",
            "Train_GAT:  0.097\n",
            "Train_GAT:  0.096\n",
            "Train_GAT:  0.096\n",
            "Train_GAT:  0.095\n",
            "Train_GAT:  0.095\n",
            "Train_GAT:  0.094\n",
            "Train_GAT:  0.094\n",
            "Train_GAT:  0.094\n",
            "Train_GAT:  0.093\n",
            "Train_GAT:  0.093\n",
            "Train_GAT:  0.093\n",
            "Train_GAT:  0.092\n",
            "Train_GAT:  0.092\n",
            "Train_GAT:  0.091\n",
            "Train_GAT:  0.091\n",
            "Train_GAT:  0.091\n",
            "Train_GAT:  0.090\n",
            "Train_GAT:  0.090\n",
            "Train_GAT:  0.089\n",
            "Train_GAT:  0.089\n",
            "Train_GAT:  0.089\n",
            "Train_GAT:  0.088\n",
            "Train_GAT:  0.088\n",
            "Train_GAT:  0.088\n",
            "Train_GAT:  0.087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize train_loss for both models. "
      ],
      "metadata": {
        "id": "oiBzIN8lRBqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#X = np.arange(0, 100, 1)\n",
        "plt.plot(train_loss_BGAT_, \"m\")\n",
        "plt.plot(train_loss_GAT_, \"c\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Train_BGAT','Train_GAT'])\n",
        "plt.title('First scenario, Train_BGAT vs Train_GAT')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cNLD0AAFznKz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "15cb78aa-6c0f-416f-b21a-995ff218178c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}