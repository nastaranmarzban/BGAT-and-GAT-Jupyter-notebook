{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount to gdrive to read data, adjacency matrix, p_link and labels. \n"
      ],
      "metadata": {
        "id": "Q8kgih540yh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hswUIQx7yDPW",
        "outputId": "c1add36d-4f75-4db3-9000-744603075f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pandas to read csv files from gdrive. Install torch and numpy which are packages that are needed durind coding."
      ],
      "metadata": {
        "id": "liD2ekI104Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViWvQz3yfvk",
        "outputId": "8ed6669d-5380-4a93-aa26-8b8e318bb670"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It is time to prepare x, edge_index, y, train_mask and test_mask which are needed for PyG. "
      ],
      "metadata": {
        "id": "KADvv2Xp1AdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1 (preparing x)**: Read multivariate dataset from gdrive and convert the type to what is needed in PyG."
      ],
      "metadata": {
        "id": "xF9Jx4eq1N6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pd.read_csv('/gdrive/MyDrive/Gene/gene_t.csv', sep=',',header=None)\n",
        "x = x.astype(np.float32) #try to cast all DataFrame columns to specified numpy.dtype\n",
        "x = torch.tensor(x.values) #convert numpy.dtype to a tensor\n",
        "print(x.sum(dim = 1).unique().size()) #to see if there is a duplicate data in x or not\n",
        "print(x.dtype)\n",
        "print(x.type())\n",
        "print(x)\n",
        "print(x.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOHBcNJgyh71",
        "outputId": "a4cae32b-418c-4514-f9ee-4b05972438b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([65])\n",
            "torch.float32\n",
            "torch.FloatTensor\n",
            "tensor([[12.2066,  8.5495, 10.7969,  ...,  8.4967,  8.5979,  8.8691],\n",
            "        [ 9.6155,  8.9314,  9.5201,  ...,  9.6052, 11.1579, 10.5243],\n",
            "        [ 6.2624, 10.6610,  8.8817,  ...,  6.8272,  6.2315,  6.5671],\n",
            "        ...,\n",
            "        [ 6.0040, 11.8118, 10.3843,  ...,  7.5903, 12.6739, 12.8235],\n",
            "        [ 8.8085, 12.8148, 15.4529,  ...,  9.6657, 13.1217, 10.3099],\n",
            "        [ 7.3143, 13.6116, 12.9968,  ...,  8.7592,  9.8862,  9.2184]])\n",
            "torch.Size([65, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2 (preparing edge_index)**: Read adjacency matrix from gdrive and convert the type to what is needed in PyG. "
      ],
      "metadata": {
        "id": "1VVXQyd-1Run"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "edge_index_csv = pd.read_csv('/gdrive/MyDrive/Gene/adj_gene.csv',sep =',',header = None)\n",
        "edge_index_numpy_ndarry = edge_index_csv.values #convert xlsx file to numpy.ndarry\n",
        "edge_index_coo = coo_matrix(edge_index_numpy_ndarry) #convert symmetric matrix to coo_matrix\n",
        "print(edge_index_coo)"
      ],
      "metadata": {
        "id": "iGhkDJWUFoai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfb7f6f-f9e2-49b5-c193-877c43662fb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 21)\t1\n",
            "  (0, 32)\t1\n",
            "  (1, 36)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 13)\t1\n",
            "  (2, 16)\t1\n",
            "  (2, 40)\t1\n",
            "  (2, 62)\t1\n",
            "  (3, 17)\t1\n",
            "  (3, 25)\t1\n",
            "  (4, 7)\t1\n",
            "  (4, 15)\t1\n",
            "  (4, 18)\t1\n",
            "  (4, 19)\t1\n",
            "  (4, 38)\t1\n",
            "  (5, 0)\t1\n",
            "  (5, 7)\t1\n",
            "  (5, 9)\t1\n",
            "  (5, 11)\t1\n",
            "  (5, 21)\t1\n",
            "  (5, 33)\t1\n",
            "  :\t:\n",
            "  (54, 59)\t1\n",
            "  (55, 36)\t1\n",
            "  (55, 46)\t1\n",
            "  (56, 15)\t1\n",
            "  (56, 42)\t1\n",
            "  (56, 64)\t1\n",
            "  (57, 8)\t1\n",
            "  (57, 23)\t1\n",
            "  (57, 25)\t1\n",
            "  (57, 38)\t1\n",
            "  (57, 54)\t1\n",
            "  (58, 21)\t1\n",
            "  (58, 59)\t1\n",
            "  (59, 54)\t1\n",
            "  (59, 58)\t1\n",
            "  (60, 13)\t1\n",
            "  (60, 44)\t1\n",
            "  (61, 35)\t1\n",
            "  (62, 2)\t1\n",
            "  (62, 48)\t1\n",
            "  (62, 64)\t1\n",
            "  (63, 34)\t1\n",
            "  (64, 18)\t1\n",
            "  (64, 56)\t1\n",
            "  (64, 62)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index_numpy = np.vstack((edge_index_coo.row, edge_index_coo.col)) #convert coo_matrix to numpy.ndarray\n",
        "edge_index_torch_int32 = torch.from_numpy(edge_index_numpy) #convert numpy.ndarray to torch.int32\n",
        "edge_index = edge_index_torch_int32.to(torch.int64) #convert torch.int32 to torch.long\n",
        "print(edge_index)\n",
        "print(edge_index.dtype)\n",
        "print(edge_index.type())\n",
        "print(edge_index.size())"
      ],
      "metadata": {
        "id": "QzTC5oZzFxr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda5b2a4-f2f6-47cb-e1ff-50b23d2272be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  0,  0,  0,  0,  0,  1,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,\n",
            "          4,  5,  5,  5,  5,  5,  5,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "          8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
            "         10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14,\n",
            "         14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 17, 17,\n",
            "         17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19,\n",
            "         19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21,\n",
            "         21, 21, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 24, 25, 25, 25, 25,\n",
            "         25, 26, 26, 26, 26, 26, 27, 27, 28, 28, 28, 29, 29, 29, 29, 29, 29, 30,\n",
            "         30, 30, 30, 30, 31, 31, 31, 31, 31, 32, 32, 32, 33, 33, 33, 33, 33, 33,\n",
            "         34, 34, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
            "         36, 37, 37, 37, 38, 38, 38, 38, 39, 40, 40, 41, 41, 42, 42, 42, 43, 43,\n",
            "         43, 44, 44, 44, 45, 45, 46, 46, 47, 48, 48, 48, 49, 50, 50, 50, 51, 52,\n",
            "         52, 52, 53, 53, 53, 53, 54, 54, 55, 55, 56, 56, 56, 57, 57, 57, 57, 57,\n",
            "         58, 58, 59, 59, 60, 60, 61, 62, 62, 62, 63, 64, 64, 64],\n",
            "        [ 5,  8,  9, 12, 21, 32, 36, 10, 13, 16, 40, 62, 17, 25,  7, 15, 18, 19,\n",
            "         38,  0,  7,  9, 11, 21, 33,  7, 19, 39,  4,  5,  6, 17, 18, 19, 24, 31,\n",
            "          0, 18, 21, 29, 38, 41, 57,  0,  5, 15, 18, 21, 24, 26, 33, 36,  2, 21,\n",
            "         36, 44,  5, 24, 26, 29, 35, 50,  0, 15, 37,  2, 14, 17, 60, 13, 17, 19,\n",
            "         47,  4,  9, 12, 16, 17, 21, 36, 56,  2, 15, 19, 26, 36, 51, 52,  3,  7,\n",
            "         13, 14, 15, 20, 26, 27, 33, 40,  4,  7,  8,  9, 36, 37, 64,  4,  6,  7,\n",
            "         14, 16, 36, 17, 21, 24, 26, 31, 38, 52,  0,  5,  8,  9, 10, 15, 20, 33,\n",
            "         37, 58, 24, 31, 36, 25, 35, 41, 57,  7,  9, 11, 20, 22,  3, 23, 30, 46,\n",
            "         57,  9, 11, 16, 17, 20, 17, 53, 29, 34, 52,  8, 11, 28, 31, 34, 50, 25,\n",
            "         31, 32, 33, 43,  7, 20, 22, 29, 30,  0, 30, 34,  5,  9, 17, 21, 30, 34,\n",
            "         28, 29, 32, 33, 63, 11, 23, 43, 61,  1,  9, 10, 15, 16, 18, 19, 22, 48,\n",
            "         55, 12, 18, 21,  4,  8, 20, 57,  6,  2, 17,  8, 23, 44, 45, 56, 30, 35,\n",
            "         53, 10, 42, 60, 42, 53, 25, 55, 14, 36, 53, 62, 50, 11, 29, 49, 16, 16,\n",
            "         20, 28, 27, 43, 45, 48, 57, 59, 36, 46, 15, 42, 64,  8, 23, 25, 38, 54,\n",
            "         21, 59, 54, 58, 13, 44, 35,  2, 48, 64, 34, 18, 56, 62]])\n",
            "torch.int64\n",
            "torch.LongTensor\n",
            "torch.Size([2, 266])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3 (preparing y)**: Read labels from gdrive and convert the type to what is needed in PyG."
      ],
      "metadata": {
        "id": "axGCXsh11WJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_csv = pd.read_csv('/gdrive/MyDrive/Gene/labels_gene.csv',sep=',',header=None)\n",
        "y = torch.tensor(labels_csv.values)\n",
        "y.resize_(65)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "Hb0Lltj8F1dk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc42887c-1511-49bd-905d-ffe7cd82bc45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step4 (preparing train_mask, test_mask)**: split x and y into train and test set."
      ],
      "metadata": {
        "id": "Tuzf5I1w1aJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "main_mask, test_mask, y_main, y_test= train_test_split(x, y,  test_size = 15, random_state = 0, shuffle = True, stratify = y)\n",
        "train_mask, val_mask, y_train, y_val= train_test_split(main_mask, y_main,  test_size= 10, random_state = 0, shuffle = True, stratify = y_main)\n",
        "print(train_mask)\n",
        "print(test_mask)\n",
        "print(train_mask.size())\n",
        "print(val_mask.size())\n",
        "print(test_mask.size())\n",
        "print(train_mask.type())\n",
        "print(train_mask.type())"
      ],
      "metadata": {
        "id": "HfW7nZ5sL4C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40ec00c-480a-422d-8acf-73526bd1b2f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10.3728,  9.5330, 10.5377,  ...,  7.4238,  6.7241,  8.8813],\n",
            "        [10.7661, 14.7948,  6.6166,  ...,  9.0051,  5.7202,  9.6194],\n",
            "        [ 6.3410,  6.3753,  6.3720,  ..., 11.3862,  6.2982, 11.8073],\n",
            "        ...,\n",
            "        [ 5.9675,  5.8466,  6.0180,  ...,  9.0986,  6.0203,  8.9446],\n",
            "        [11.8756,  6.9120,  6.6157,  ..., 11.1637, 13.5823, 15.8961],\n",
            "        [ 6.0380,  5.8927,  6.0503,  ...,  5.9728,  5.9658,  6.9342]])\n",
            "tensor([[ 8.2385,  8.0172,  8.6788,  5.8845,  7.5658,  9.9586,  7.6901,  9.1844,\n",
            "          7.8667,  7.3978,  6.1370,  6.2020,  7.4180,  8.9171,  7.2701,  7.8511,\n",
            "          5.9772,  9.4203,  8.0586, 10.9160,  6.7655, 10.1374, 10.4430,  8.2356,\n",
            "          7.6936,  7.6223,  8.2016,  8.4035,  8.9126,  8.4068,  8.5712,  9.5520,\n",
            "          8.6565,  8.8856, 10.1591,  6.0529,  8.3346,  8.1694,  7.0149,  8.4008,\n",
            "          9.2275,  8.0515,  7.6185,  8.2873,  7.3589,  7.4822,  6.9951,  7.7066,\n",
            "          7.0688,  8.1530, 10.7130,  7.1828,  5.9988,  6.9255,  6.4817,  6.7041,\n",
            "          8.5162,  7.6118,  6.2231,  9.6226],\n",
            "        [ 6.5182,  8.7490, 13.6674, 14.3974, 15.0252, 12.3609, 13.7553,  7.8490,\n",
            "         13.9711, 13.5721, 15.2875, 14.8780, 14.0020, 14.1202, 14.2938, 13.7621,\n",
            "         12.8963,  8.8955, 14.5730, 14.1855,  7.0996, 11.9012, 13.1206, 13.4968,\n",
            "         11.3230,  9.7123, 14.0917, 12.9887, 10.8368, 14.0471, 12.8529,  7.5424,\n",
            "         14.9212,  6.4189, 11.2763, 14.6053, 14.0272, 13.6352, 14.3648, 13.8291,\n",
            "         11.0142, 14.3720, 10.3710, 14.6372, 12.7671, 11.8467, 13.4448,  9.3569,\n",
            "          6.3472, 12.4210,  7.2908,  7.9082, 14.1321, 13.0283, 13.5557, 13.3261,\n",
            "         14.4061, 14.8179, 14.6373, 13.8883],\n",
            "        [11.1470,  9.6138,  8.4386,  8.6996,  7.9425, 11.6792, 14.7668, 12.3094,\n",
            "         13.5950, 10.2458,  9.0299,  8.8998, 14.1635, 11.3266,  8.3800, 13.8652,\n",
            "         12.6317,  7.2759,  6.7465, 11.2667,  7.3391,  5.1540, 12.6853, 11.2388,\n",
            "         12.4668,  5.1094,  8.1739,  7.1444,  9.9468,  9.0264, 12.0875, 13.3078,\n",
            "         13.7746,  5.8485,  5.3039, 11.0104, 13.3267, 13.7210,  9.3786, 13.7331,\n",
            "          8.8444, 14.8793,  6.6086,  7.2980, 11.3095,  8.4718,  5.0999,  9.2888,\n",
            "          9.6658,  8.0610, 11.7746, 12.8773, 10.7858,  9.3472, 11.1577,  6.7443,\n",
            "         12.5258, 10.4380, 13.5641, 11.5209],\n",
            "        [10.7183,  9.1478,  9.2247,  9.9046,  7.5782, 10.1151, 10.1096,  8.9446,\n",
            "         10.0111,  8.0175,  6.3471,  7.8571,  9.5947,  6.5731,  7.1938, 10.1144,\n",
            "          8.0851,  7.5910,  6.8466,  8.1262,  8.7188,  8.6907,  9.0671, 10.3353,\n",
            "          8.0145,  8.5375,  7.9349,  9.9537,  8.6885,  8.3069,  9.0204, 10.3729,\n",
            "          8.5851, 10.2359,  6.4739,  9.0938,  6.7975,  6.9006,  8.2494,  8.7005,\n",
            "          6.3753,  6.3311,  8.4629,  9.0074,  6.0224,  8.8102,  6.2959, 11.6744,\n",
            "         11.1184,  9.2387, 10.7843,  8.1389,  7.9195,  7.6718,  9.5289, 10.6334,\n",
            "          6.2283,  6.9559,  6.4644,  7.6421],\n",
            "        [ 6.2350,  8.9986, 14.0799, 13.3041,  9.6946,  7.0157, 13.2853,  7.3008,\n",
            "          7.9758, 10.4391, 14.0352, 15.3024,  6.1368, 13.3989,  6.2802, 10.6703,\n",
            "          6.2696,  7.4487,  6.9769, 10.3241, 15.0338, 13.2090, 13.7001, 12.5107,\n",
            "         15.3559,  8.8233, 14.9755, 14.0102,  6.2155,  6.2213, 11.0928, 11.9061,\n",
            "         12.3741, 11.7275, 12.1490, 10.7724, 11.6645, 12.8503, 12.7653, 12.9430,\n",
            "          7.0827, 11.0000, 10.6046, 11.6170,  6.1483,  8.9403, 13.4391, 13.8186,\n",
            "         11.4947, 10.8933,  6.2092,  7.0782,  6.1566, 13.0519,  6.1279,  6.9781,\n",
            "          8.8461, 14.3516,  6.2356, 13.4448],\n",
            "        [12.2066,  8.5495, 10.7969, 11.1926,  9.1050, 11.7446, 10.9571, 11.3216,\n",
            "          8.4799,  8.7302,  7.4201,  8.5333,  9.2753,  9.3547,  8.7739, 11.9748,\n",
            "          8.7563,  8.8228,  8.4775,  8.7904,  8.5630, 10.7945, 11.4844, 10.4335,\n",
            "          9.3719,  8.8170,  9.2389, 10.6604, 10.7973,  7.6846,  8.8924, 11.0579,\n",
            "          9.8199, 12.2061,  7.3871,  9.8332,  8.8557,  7.9136,  8.3253,  9.2273,\n",
            "          8.3398,  7.9132,  8.3345,  9.7002,  7.4594,  7.6213,  8.5847, 10.1583,\n",
            "          8.2436,  9.1243, 11.9950,  9.7610,  9.5282,  9.9043,  9.6816,  9.1900,\n",
            "          7.8349,  8.4967,  8.5979,  8.8691],\n",
            "        [ 9.1704,  5.5303,  7.8041,  5.0921,  6.3536, 13.0301,  6.9353,  5.1501,\n",
            "          6.0166,  8.5868, 10.6643,  7.7030, 13.2391, 12.9548, 10.0902, 13.4239,\n",
            "          5.2282, 14.3567, 11.8293, 13.4292,  8.5222, 11.7371, 13.4268, 11.5504,\n",
            "          8.6717,  7.9192,  9.8456,  8.4701,  6.5384,  7.9176,  5.1007, 10.7804,\n",
            "         11.5593,  8.2603, 12.8679, 11.4248,  7.9661,  6.5802, 12.5804,  5.2083,\n",
            "          9.2188,  8.4033,  8.0019, 10.7967, 11.6547, 14.3449,  8.2412,  7.7924,\n",
            "         12.9295,  7.7841, 12.4068, 13.2547, 10.2028, 11.3653,  7.3620,  8.8106,\n",
            "          7.7085, 11.0134,  9.4556, 11.3647],\n",
            "        [ 8.3129, 12.5604, 11.7595,  8.5082, 12.4951, 12.4364, 10.5157, 11.3609,\n",
            "          9.7897, 10.8277,  9.5494,  8.6944, 11.0301, 10.3457, 11.5092,  9.8279,\n",
            "          8.3191, 11.0563, 10.5749, 10.5534,  8.9745, 11.9298,  9.0355,  8.7520,\n",
            "         10.9828,  9.5276, 10.5698,  9.1369, 11.3201,  8.7487, 11.1828, 11.1211,\n",
            "         10.5110,  9.1372, 11.8887, 10.8368,  8.0538, 11.3790, 10.8540, 11.0434,\n",
            "          7.8369, 11.8591,  8.7038, 10.5830,  8.1342, 10.7071, 11.1617, 11.9868,\n",
            "         10.6502, 10.6453, 10.4590,  9.4723,  8.7417,  9.7220, 11.1146, 11.8264,\n",
            "         10.9802,  8.0078,  8.2029,  9.2508],\n",
            "        [ 8.6346, 13.0821, 13.3994, 13.8197,  7.1482, 11.4006,  8.3059, 11.4370,\n",
            "         13.4914, 13.7286, 14.7704,  6.6662, 10.1378,  9.4849,  8.5306,  9.5501,\n",
            "         10.0055,  6.4275, 12.0844,  8.7048, 13.0089, 10.6141, 12.8063,  8.3891,\n",
            "          6.7053, 12.3551,  8.5216, 14.6901, 10.1561, 10.9684, 11.2551, 10.0703,\n",
            "          9.8989,  7.3388,  6.5744, 13.6116, 14.2124, 12.6795,  6.0371, 10.5109,\n",
            "         12.0161,  6.0158, 10.1612, 13.2455,  8.7601, 10.7800, 15.9501,  9.8713,\n",
            "          6.7188, 12.1531, 14.2038, 13.9298,  6.4309, 10.7978,  7.7693, 13.7478,\n",
            "         15.1711, 15.4459, 14.7873,  6.2878],\n",
            "        [14.1577,  5.5685,  9.4507,  8.2329,  6.2866,  9.7270, 13.9929, 10.5504,\n",
            "          5.5002, 13.9826,  8.1254,  8.9273,  6.7370, 14.9201,  6.5104, 11.1630,\n",
            "          8.3298,  9.7252,  9.9552,  5.6665, 13.1270, 12.8754,  6.0064,  5.9086,\n",
            "          5.9562,  7.8113,  6.4834, 14.3173, 11.8457,  9.6335, 14.3144, 13.2208,\n",
            "          5.6262, 11.4276, 12.3831, 11.7474, 14.6004,  8.5693,  5.3714, 13.4733,\n",
            "         14.8287,  8.2055,  5.7925, 13.0263, 14.8026, 12.2479,  9.4577,  7.4880,\n",
            "          7.0409,  5.3468,  5.4813, 14.4246,  7.1641, 12.4784, 11.3679, 11.0362,\n",
            "         10.3765, 14.7559,  9.4372,  5.1968],\n",
            "        [ 6.0486,  6.0949,  7.7339,  6.0939,  8.1846,  7.8816,  8.4513,  6.1879,\n",
            "          7.8949,  6.9687,  6.0605,  8.8580,  8.8027,  7.6362,  8.6914,  6.0834,\n",
            "          6.0708,  6.2821,  6.8710,  8.6752,  8.1127,  6.1834,  6.1093,  6.2882,\n",
            "          6.3808,  6.3941,  6.0584,  8.2212,  5.9565,  6.0445,  6.9414,  6.1709,\n",
            "          6.2241,  9.6152,  5.9208,  6.1911,  8.7467,  8.9756,  6.8237,  7.6841,\n",
            "          6.2041,  6.7963,  6.1182,  7.4578,  6.0079,  6.4344,  6.1767,  8.1251,\n",
            "          9.4367,  6.2793,  6.0929,  9.1364,  9.4025,  6.0803,  6.0662,  6.1777,\n",
            "          9.1353,  6.5167,  8.5982,  8.3380],\n",
            "        [15.3854,  6.1554,  6.3643,  6.1616, 10.6098,  6.0488, 10.4535,  6.3259,\n",
            "         14.7020,  7.1673, 13.4898,  7.2520,  7.9522, 13.0392,  6.2052,  6.1731,\n",
            "          8.5788,  7.3517,  8.7601,  6.9821,  6.1318,  8.6157,  6.1512,  9.2539,\n",
            "          7.7882,  6.2391, 12.8662,  7.2310, 14.4037, 10.8922,  9.6585,  6.3398,\n",
            "          6.7328,  6.2359,  9.0409,  6.8926,  6.3007,  6.7214, 11.7808,  9.0264,\n",
            "          9.6532,  6.2362,  6.4779, 15.2910,  6.3419,  6.2924,  9.6496,  6.1857,\n",
            "          6.1710, 10.7096,  6.1335,  6.5555,  6.0633,  6.2153,  6.2663,  8.5757,\n",
            "          6.4725, 12.2267, 14.7258, 12.2971],\n",
            "        [ 6.6363,  6.4984,  6.9222,  6.7871,  7.5963,  7.0077,  9.3584,  6.5291,\n",
            "          6.5776,  7.8834,  8.9515, 11.1117,  7.1605, 10.2193,  8.5122,  6.5895,\n",
            "          7.2393,  6.9016, 10.1774,  9.7566,  6.1657,  7.3825,  6.9058,  7.3013,\n",
            "          9.9066,  8.1451,  8.4803,  6.9156,  7.4020,  8.8510,  6.8273,  7.6321,\n",
            "          9.2422,  6.5081,  6.5617,  7.3187, 10.8334,  8.6882,  7.3173,  6.5779,\n",
            "          9.2137,  9.3201,  7.8518,  7.6423,  8.7952,  6.3739,  7.1334,  6.7544,\n",
            "          6.5073,  7.0175,  7.8585, 11.4001,  6.5192,  7.4071,  7.1492,  7.5229,\n",
            "          8.0391,  9.4433,  8.3413, 10.1722],\n",
            "        [13.2876, 12.2524,  9.4542, 12.2720, 12.8178,  5.9645,  8.2121,  6.5243,\n",
            "         13.3332,  8.8836, 10.0935,  8.8953,  9.9275,  7.5088,  9.7069,  6.1138,\n",
            "         13.9716,  7.1695,  5.9130,  8.1810,  5.5485, 12.7754, 12.6163,  7.8212,\n",
            "          5.5915,  7.7659, 11.0820,  8.2876, 12.3036,  9.4850,  9.9241,  7.4679,\n",
            "         14.1055,  8.5660, 12.2167,  6.2915, 13.0153,  9.1757,  5.8442, 11.1285,\n",
            "          6.7240, 14.0520, 14.4907,  9.1315, 13.6134, 11.8353,  5.3156,  8.2772,\n",
            "         11.1364,  8.2779,  8.7832, 11.2620,  7.2243, 14.7708, 12.0018,  5.5940,\n",
            "         11.4677, 14.7161, 11.7213, 12.5715],\n",
            "        [ 6.1420,  7.0517,  6.3356,  6.1135,  7.3391,  9.2887,  6.2315,  8.5438,\n",
            "          9.1296,  9.4795,  7.6617,  8.5541,  6.1446,  6.1965,  6.3025,  8.4849,\n",
            "          9.3310,  6.0667,  6.1547,  6.1617,  7.2672,  6.1082,  6.4202,  8.7842,\n",
            "          6.9633,  7.6564,  6.6337,  8.3342,  6.1422,  8.4383,  6.1323,  6.5174,\n",
            "          9.1843,  6.0249,  6.0717,  5.9916,  7.7627,  6.7497,  6.6422,  6.1398,\n",
            "          6.7720,  6.0471,  7.9435,  6.1271,  6.1102,  5.9816,  6.0164,  5.9754,\n",
            "          6.2812,  6.2263, 10.6284,  6.1668,  8.1517,  6.7646,  7.2735,  6.1009,\n",
            "          7.3298,  8.2965,  6.8311,  8.6497]])\n",
            "torch.Size([40, 60])\n",
            "torch.Size([10, 60])\n",
            "torch.Size([15, 60])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To see how many percent of data belong to each class."
      ],
      "metadata": {
        "id": "a-YBIXMH4lBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels, counts = np.unique(y, return_counts = True)\n",
        "print(counts/float(len(y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJ8sEuT2hQq",
        "outputId": "a0105dc8-6027-4810-b761-99bc6af70ca1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33846154 0.32307692 0.33846154]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels, counts = np.unique(y_train, return_counts = True)\n",
        "print(counts/float(len(y_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms7xLrVx2miG",
        "outputId": "cdf90fb2-e524-44bf-8344-71a778e083c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.325 0.325 0.35 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels, counts = np.unique(y_test, return_counts = True)\n",
        "print(counts/float(len(y_val)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y14miOm52sQf",
        "outputId": "5848f490-87c3-475c-e9f5-0f64b28cc265"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5 0.5 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels, counts = np.unique(y_test, return_counts = True)\n",
        "print(counts/float(len(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7bAnq542wLI",
        "outputId": "e14d1bcf-9fc6-45c4-e5bf-4653cb008e20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33333333 0.33333333 0.33333333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing train_mask and test_mask based on PyG."
      ],
      "metadata": {
        "id": "l0QyEvRQ2zl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = (x.unsqueeze(0) == train_mask.unsqueeze(1)).all(dim=2).any(dim=0)\n",
        "print(train_mask)\n",
        "print(train_mask.size())\n",
        "print(sum(train_mask))\n",
        "\n",
        "val_mask = (x.unsqueeze(0) == val_mask.unsqueeze(1)).all(dim=2).any(dim=0)\n",
        "print(val_mask)\n",
        "print(val_mask.size())\n",
        "print(sum(val_mask))\n",
        "\n",
        "test_mask = (x.unsqueeze(0) == test_mask.unsqueeze(1)).all(dim=2).any(dim=0)\n",
        "print(test_mask)\n",
        "print(test_mask.size())\n",
        "print(sum(test_mask))"
      ],
      "metadata": {
        "id": "LSTDhRO1MJQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ec56dd-a152-4bfc-e7cb-2ae3c8b5ce11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([False,  True,  True, False,  True,  True, False, False, False, False,\n",
            "         True,  True,  True,  True,  True, False,  True, False,  True, False,\n",
            "         True,  True, False,  True,  True,  True,  True, False, False, False,\n",
            "        False, False,  True,  True,  True,  True,  True, False, False,  True,\n",
            "         True, False, False,  True, False,  True, False,  True, False, False,\n",
            "         True,  True,  True,  True, False, False,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True])\n",
            "torch.Size([65])\n",
            "tensor(40)\n",
            "tensor([False, False, False, False, False, False,  True, False,  True,  True,\n",
            "        False, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False,  True, False, False, False, False, False, False,  True,\n",
            "        False, False, False, False, False, False, False,  True, False, False,\n",
            "        False, False, False, False,  True, False, False, False,  True, False,\n",
            "        False, False, False, False, False,  True, False, False, False, False,\n",
            "        False, False, False, False, False])\n",
            "torch.Size([65])\n",
            "tensor(10)\n",
            "tensor([ True, False, False,  True, False, False, False,  True, False, False,\n",
            "        False, False, False, False, False,  True, False, False, False,  True,\n",
            "        False, False, False, False, False, False, False,  True,  True, False,\n",
            "         True,  True, False, False, False, False, False, False,  True, False,\n",
            "        False,  True,  True, False, False, False,  True, False, False,  True,\n",
            "        False, False, False, False,  True, False, False, False, False, False,\n",
            "        False, False, False, False, False])\n",
            "torch.Size([65])\n",
            "tensor(15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It is time to use p_link as attentions."
      ],
      "metadata": {
        "id": "8tTHujoz1pZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_link = pd.read_csv('/gdrive/MyDrive/Gene/p_links_gene.csv', sep=',',header=None)\n",
        "p_link = p_link.astype(np.float32)\n",
        "p_link = torch.tensor(p_link.values)\n",
        "#p_link = p_link.to(torch.float32)\n",
        "print(p_link.size())\n",
        "print(p_link.dtype)\n",
        "print(p_link.type())"
      ],
      "metadata": {
        "id": "YnomEV3QMPwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d44eef0-5349-480d-c3ed-b340c8ab7163"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([65, 65])\n",
            "torch.float32\n",
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now everything is ready to develop our GNNs."
      ],
      "metadata": {
        "id": "7OJepfTR1tho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install torch_geometric."
      ],
      "metadata": {
        "id": "vQkS1C1848bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "N7NhPODCMdk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828e7c29-f0df-4ad3-8350-5d94eb3d64f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 13.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 280 kB 10.0 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "Data = Data(x=x, edge_index=edge_index, y = y, train_mask = train_mask, val_mask = val_mask, test_mask = test_mask)\n",
        "print(Data)"
      ],
      "metadata": {
        "id": "Us1qk4AJMkQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8575a6-0df6-4b7d-e90e-e880a242ed4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[65, 60], edge_index=[2, 266], y=[65], train_mask=[65], val_mask=[65], test_mask=[65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save data in order to use it. "
      ],
      "metadata": {
        "id": "7lido70L1gXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_save_name = 'gene.pt'  \n",
        "path = F\"/gdrive/MyDrive/Gene/{data_save_name}\" \n",
        "torch.save(Data.to_dict(), path)"
      ],
      "metadata": {
        "id": "gwuv2xfbMsEE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_link_save_name = 'p_links_gene.pt'  \n",
        "path = F\"/gdrive/MyDrive/Gene/{p_link_save_name}\" \n",
        "torch.save(p_link, path)"
      ],
      "metadata": {
        "id": "0NC5-ykDMxtm"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}